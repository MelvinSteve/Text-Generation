{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBO78ScK3FFx"
      },
      "source": [
        "<center><h1>Introduction to Word Level Language Modelling(Practical Implementation)</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGtLLB_ADRl_"
      },
      "source": [
        "---\n",
        "# **Table of Contents**\n",
        "---\n",
        "\n",
        "**1.** [**Introduction**](#Section1)<br>\n",
        "**2.** [**Problem Description**](#Section2)<br>\n",
        "**3.** [**Installing & Importing Libraries**](#Section3)<br>\n",
        "**4.** [**Data Acquisition & Description**](#Section4)<br>\n",
        "**5.** [**Data Preprocessing**](#Section5)<br>\n",
        "**6.** [**Train Language Model**](#Section6)<br>\n",
        "  - **6.1** [**Load Sequence**](#Section61)\n",
        "  - **6.2** [**Encode Sequence**](#Section62)\n",
        "  - **6.3** [**Sequence Inputs and Output**](#Section63)\n",
        "  - **6.4** [**Fit Model**](#Section61)\n",
        "\n",
        "**7.** [**Use Language Model**](#Section7)<br>\n",
        "  - **7.1** [**Load Sequence**](#Section61)\n",
        "  - **7.2** [**Load Model**](#Section62)\n",
        "  - **7.3** [**Fit Model**](#Section63)\n",
        "\n",
        "**8.** [**Conclusion**](#Section8)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt3g_Q2dZJwN"
      },
      "source": [
        "---\n",
        "<a name = Section1></a>\n",
        "# **1. Introduction**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dVcPtmpNiZJ"
      },
      "source": [
        "- **Language models** learn and **predict** one word at a time. The **training** of the network involves **providing** sequences of words as **input** that are processed one at a time where a **prediction** can be made and learned for each **input sequence**.\n",
        "\n",
        "- Neural Language Models (NLM) address the **N-gram data sparsity** issue through **parameterization** of words as **vectors** (word embeddings) and using them as inputs to a neural network.\n",
        "\n",
        "- Word **embeddings** obtained through NLMs **exhibit** the **property** whereby semantically close **words** are likewise **close** in the induced **vector space**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIpcO8AsTU1i"
      },
      "source": [
        "---\n",
        "<a name = Section2></a>\n",
        "# **2. Problem Statement**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B523lFJxHXT6"
      },
      "source": [
        "- The **problem statement** is to train a **language model** on the given text and then **generate** text given an input text in such a way that it looks **straight** out of this document and is **grammatically** correct and **legible** to read.\n",
        "\n",
        "* For this, we need to develop **word-level** neural language **model** and use  it to generate text.\n",
        "\n",
        "* A **language model** can predict the probability of the next word in the sequence, based on the **words** already **observed** in the sequence.\n",
        "\n",
        "* **Neural network models** are a preferred method for **developing statistical language models** because they can use a **distributed representation** where different words with similar meanings have **similar representation**.\n",
        "\n",
        "- Also, it is because they can use a **large context** of recently observed words when **making predictions**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOmVgY8yWE36"
      },
      "source": [
        "---\n",
        "<a name = Section3></a>\n",
        "# **3. Installing and Importing Libraries**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItkIWd-QuyJn",
        "outputId": "d05d515e-594c-42aa-a225-1d4343225829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "# Import tensorflow 2.x\n",
        "# This code block will only work in Google Colab.\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYgJy8N1Gpd2"
      },
      "outputs": [],
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9OWuHO0WF1n"
      },
      "source": [
        "---\n",
        "<a name = Section4></a>\n",
        "# **4. Data Acquisition & Description**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iIrOcT-Fn6N"
      },
      "source": [
        "- **The Republic by Plato**\n",
        "<br>\n",
        "<center> <img src=\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/socrates.JPG\" /></center>\n",
        "<br>\n",
        "\n",
        "-  The Republic is the **classical Greek philosopher Platoâ€™s** most famous work.\n",
        "\n",
        "- It is **structured** as a **dialog** (e.g. conversation) on the topic of **order and justice** within a city state\n",
        "\n",
        "- Download the ASCII **text version** of the entire book (or books) here: [The Republic](https://https://www.gutenberg.org/ebooks/1497) and save it as *republic.txt*\n",
        "\n",
        "- Open the file in a **text editor** and delete the **front** and **back** matter.\n",
        "\n",
        "- This includes details about the **book** at the beginning, a **long analysis**, and **license** information at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq0Ve7ItvsQP",
        "outputId": "8689c6f1-3d3b-4606-b5cc-52a8dc3da678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rBOOK I.\r\r\n",
            "\r\r\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\r\r\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\r\r\n",
            "Artemis.); and also because I wanted to see in what manner they would\r\r\n",
            "celebrate the festival, which was a new thing. I was delighted with the\r\r\n",
            "procession of the inhabitants; but that of the Thracians was equally,\r\r\n",
            "if not more, beautiful. When we had finished our prayers and viewed the\r\r\n",
            "spectacle, we turned in the direction of the city; and at that instant\r\r\n",
            "Polemarchus the son of Cephalus chanced to catch sight of us from a\r\r\n",
            "distance as we were starting on our way home, and told his servant to\r\r\n",
            "run and bid us wait for him. The servant took hold of me by the cloak\r\r\n",
            "behind, and said: Polemarchus desires you to wait.\r\r\n",
            "\r\r\n",
            "I turned round, and asked him where his master was.\r\r\n",
            "\r\r\n",
            "There he is, said the youth, coming after you, if you will only wait.\r\r\n",
            "\r\r\n",
            "Certainly we will, said Glaucon; and in a few minutes Polemarchus\r\r\n",
            "appeared, and with him Adeimantus, Glaucon's brother, Niceratus the son\r\r\n",
            "of Nicias, and several others who had been at the procession.\r\r\n",
            "\r\r\n",
            "Polemarchus said to me: I perceive, Socrates, that you and your\r\r\n",
            "companion are already on your way to the city.\r\r\n",
            "\r\r\n",
            "You are not far wrong, I said.\r\r\n",
            "\r\r\n",
            "But do you see, he rejoined, how many we are?\r\r\n",
            "\r\r\n",
            "Of course.\r\r\n",
            "\r\r\n",
            "And are you stronger than all these? for if not, you will have to remain\r\r\n",
            "where you are.\r\r\n",
            "\r\r\n",
            "May there not be the alternative, I said, that we may persuade you to\r\r\n",
            "let us go?\r\r\n",
            "\r\r\n",
            "But can you persuade us, if we refuse to listen to you? he said.\r\r\n",
            "\r\r\n",
            "Certainly not, replied Glaucon.\r\r\n",
            "\r\r\n",
            "Then we are not going to listen; of that you may be assured.\r\r\n",
            "\r\r\n",
            "Adeimantus added: Has no one told you of the torch-race on horseback in\r\r\n",
            "honour of the goddess which will take place in the evening?\r\r\n",
            "\r\r\n",
            "With horses! I replied: That is a novelty. Will horsemen carry torches\r\r\n",
            "and pass them one to another during the race?\r\r\n",
            "\r\r\n",
            "Yes, said Polemarchus, an\n"
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "response = urllib.request.urlopen('https://raw.githubusercontent.com/insaid2018/DeepLearning/master/Data/republic_clean.txt')\n",
        "doc = response.read().decode('utf8')\n",
        "print(doc[:2000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lszB-fmkWW4f"
      },
      "source": [
        "---\n",
        "<a name = Section5></a>\n",
        "# **5. Data Preprocessing**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvysmaT93FF6"
      },
      "source": [
        "We'll be using the following **process sequence** in this notebook:\n",
        "\n",
        "<br>   \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/word_lstm_flow0.png\"width=\"600\" height=\"400\"/></center>\n",
        "\n",
        "<br>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKSbwDDO3FGU"
      },
      "source": [
        "\n",
        "#### Clean Text\n",
        "\n",
        "* **Replace â€˜â€“â€˜** with a white space so we can split words better.\n",
        "\n",
        "* **Split words** based on **white space**.\n",
        "\n",
        "* Remove all **punctuation** from **words** to reduce the vocabulary size (e.g. â€˜What?â€™ becomes â€˜Whatâ€™).\n",
        "\n",
        "* **Remove all words** that are not alphabetic to remove standalone **punctuation tokens**.\n",
        "\n",
        "* Normalize **all words** to **lowercase** to reduce the **vocabulary size**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCORHQHPGmTC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "    doc = doc.replace('--', ' ')\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPb4XvVCGslQ",
        "outputId": "1724062e-62f7-4904-c011-0aad932ee453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n",
            "Total Tokens: 118684\n",
            "Unique Tokens: 7409\n"
          ]
        }
      ],
      "source": [
        "# clean document\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOI6sfwaG8QB",
        "outputId": "88b42719-1579-4c84-b569-84631d1f36bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Sequences: 118633\n"
          ]
        }
      ],
      "source": [
        "# organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "    # select sequence of tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # convert into a line\n",
        "    line = ' '.join(seq)\n",
        "    # store\n",
        "    sequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "b5YCAlga3FGw",
        "outputId": "3b7480be-6287-4fbe-f46e-4d07b2c5f80c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequences[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf7dLooF3FGp"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "- Transforming the tokens into **space-separated strings** for later storage in a file.\n",
        "\n",
        "- Splitting the list of **clean tokens** into **sequences**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RksKge0_G_Oj"
      },
      "outputs": [],
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "# save sequences to file\n",
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IwlwV4_H3OW"
      },
      "source": [
        "----\n",
        "<a id=section6></a>\n",
        "## **6. Train Language Model**\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDXpKVrv3FG6"
      },
      "source": [
        "\n",
        "<br>   \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/word_lstm_flow4.png\"width=\"700\" height=\"400\"/></center>\n",
        "\n",
        "<br>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHCFMESq3FG7"
      },
      "source": [
        "* Model uses a **distributed** representation for words so that different words with similar meanings will have a similar representation.\n",
        "\n",
        "* It **learns** the **representation** at the same time as **learning the model.**\n",
        "\n",
        "* It **learns** to predict the **probability** for the next **word** using the **context** of the last **100 words**.\n",
        "\n",
        "- We will use an **Embedding Layer** to learn the representation of words, and a **Long Short-Term Memory (LSTM)** recurrent neural network to learn to **predict words** based on their context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NmWD7bb3FG_"
      },
      "source": [
        "<a id=section601></a>\n",
        "### **6.1 Load Sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBgoos1p3FHB"
      },
      "source": [
        "- We can load our **training data** using the **`load_doc()`** function defined below.\n",
        "\n",
        "\n",
        "- Once loaded, we can **split the data into separate training sequences** by splitting based on new lines.\n",
        "\n",
        "\n",
        "- The snippet below will load the **â€˜republic_sequences.txtâ€˜** data file from the current working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYZIaBv-IRDJ"
      },
      "outputs": [],
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7kz_wjL3FHH",
        "outputId": "adf96ca9-2a85-41a3-fa04-9930a699b3c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was',\n",
              " 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhFPp4lxIW1E"
      },
      "source": [
        "<a id=section602></a>\n",
        "### **6.2 Encode Sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdcRYdZI3FHN"
      },
      "source": [
        "- The **word embedding layer** expects input sequences to be comprised of integers.\n",
        "\n",
        "- We can **map each word in our vocabulary** to a unique integer and encode our input sequences.\n",
        "\n",
        "- Later, when we make predictions, we can convert the **prediction to numbers** and look up their **associated words** in the **same mapping**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDn6Si02IeI6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "First, the Tokenizer must be trained on the entire training dataset, which means it finds all of the unique words in the data and assigns each a unique integer.\n",
        "\n",
        "We can then use the fit Tokenizer to encode all of the training sequences, converting each sequence from a list of words to a list of integers.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjVfBSmPpOjE",
        "outputId": "28945033-a1ca-47ed-85ef-6e6ae9f659b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxR_DnR9I7Zt"
      },
      "source": [
        "- We can access the **mapping** of **words** to **integers** as a dictionary attribute called **`word_index`** on the **tokenizer** object.\n",
        "\n",
        "- We need to know the **size** of the **vocabulary** for defining the **embedding** layer later.\n",
        "\n",
        "- We can determine the vocabulary by **calculating** the size of the **mapping dictionary**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2ZZJ3DxI_2-",
        "outputId": "fd92698f-83c3-48c9-e4b0-d3f48fd0b5bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7410"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# vocabulary sizeb\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0xkGaniol60",
        "outputId": "f602a222-998e-4de9-b3d1-1a4522bf4c5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'and': 2,\n",
              " 'of': 3,\n",
              " 'to': 4,\n",
              " 'is': 5,\n",
              " 'in': 6,\n",
              " 'he': 7,\n",
              " 'a': 8,\n",
              " 'that': 9,\n",
              " 'be': 10,\n",
              " 'i': 11,\n",
              " 'not': 12,\n",
              " 'which': 13,\n",
              " 'are': 14,\n",
              " 'you': 15,\n",
              " 'they': 16,\n",
              " 'or': 17,\n",
              " 'will': 18,\n",
              " 'said': 19,\n",
              " 'as': 20,\n",
              " 'we': 21,\n",
              " 'but': 22,\n",
              " 'have': 23,\n",
              " 'them': 24,\n",
              " 'his': 25,\n",
              " 'for': 26,\n",
              " 'by': 27,\n",
              " 'who': 28,\n",
              " 'their': 29,\n",
              " 'what': 30,\n",
              " 'then': 31,\n",
              " 'this': 32,\n",
              " 'one': 33,\n",
              " 'if': 34,\n",
              " 'with': 35,\n",
              " 'there': 36,\n",
              " 'all': 37,\n",
              " 'true': 38,\n",
              " 'at': 39,\n",
              " 'when': 40,\n",
              " 'do': 41,\n",
              " 'other': 42,\n",
              " 'has': 43,\n",
              " 'yes': 44,\n",
              " 'any': 45,\n",
              " 'him': 46,\n",
              " 'no': 47,\n",
              " 'good': 48,\n",
              " 'would': 49,\n",
              " 'may': 50,\n",
              " 'state': 51,\n",
              " 'from': 52,\n",
              " 'man': 53,\n",
              " 'say': 54,\n",
              " 'our': 55,\n",
              " 'only': 56,\n",
              " 'was': 57,\n",
              " 'an': 58,\n",
              " 'must': 59,\n",
              " 'should': 60,\n",
              " 'so': 61,\n",
              " 'more': 62,\n",
              " 'us': 63,\n",
              " 'can': 64,\n",
              " 'on': 65,\n",
              " 'were': 66,\n",
              " 'very': 67,\n",
              " 'now': 68,\n",
              " 'like': 69,\n",
              " 'such': 70,\n",
              " 'replied': 71,\n",
              " 'just': 72,\n",
              " 'certainly': 73,\n",
              " 'than': 74,\n",
              " 'also': 75,\n",
              " 'these': 76,\n",
              " 'men': 77,\n",
              " 'same': 78,\n",
              " 'another': 79,\n",
              " 'about': 80,\n",
              " 'justice': 81,\n",
              " 'own': 82,\n",
              " 'how': 83,\n",
              " 'soul': 84,\n",
              " 'been': 85,\n",
              " 'let': 86,\n",
              " 'into': 87,\n",
              " 'being': 88,\n",
              " 'shall': 89,\n",
              " 'it': 90,\n",
              " 'most': 91,\n",
              " 'my': 92,\n",
              " 'me': 93,\n",
              " 'nature': 94,\n",
              " 'whether': 95,\n",
              " 'life': 96,\n",
              " 'had': 97,\n",
              " 'many': 98,\n",
              " 'those': 99,\n",
              " 'things': 100,\n",
              " 'some': 101,\n",
              " 'way': 102,\n",
              " 'mean': 103,\n",
              " 'knowledge': 104,\n",
              " 'well': 105,\n",
              " 'first': 106,\n",
              " 'your': 107,\n",
              " 'make': 108,\n",
              " 'know': 109,\n",
              " 'see': 110,\n",
              " 'her': 111,\n",
              " 'out': 112,\n",
              " 'think': 113,\n",
              " 'evil': 114,\n",
              " 'right': 115,\n",
              " 'truth': 116,\n",
              " 'himself': 117,\n",
              " 'whom': 118,\n",
              " 'sort': 119,\n",
              " 'because': 120,\n",
              " 'quite': 121,\n",
              " 'never': 122,\n",
              " 'up': 123,\n",
              " 'unjust': 124,\n",
              " 'better': 125,\n",
              " 'injustice': 126,\n",
              " 'reason': 127,\n",
              " 'others': 128,\n",
              " 'saying': 129,\n",
              " 'either': 130,\n",
              " 'therefore': 131,\n",
              " 'nothing': 132,\n",
              " 'every': 133,\n",
              " 'am': 134,\n",
              " 'two': 135,\n",
              " 'great': 136,\n",
              " 'best': 137,\n",
              " 'opinion': 138,\n",
              " 'city': 139,\n",
              " 'suppose': 140,\n",
              " 'far': 141,\n",
              " 'take': 142,\n",
              " 'ought': 143,\n",
              " 'having': 144,\n",
              " 'both': 145,\n",
              " 'question': 146,\n",
              " 'time': 147,\n",
              " 'made': 148,\n",
              " 'upon': 149,\n",
              " 'why': 150,\n",
              " 'thing': 151,\n",
              " 'cannot': 152,\n",
              " 'art': 153,\n",
              " 'again': 154,\n",
              " 'part': 155,\n",
              " 'order': 156,\n",
              " 'gods': 157,\n",
              " 'after': 158,\n",
              " 'mind': 159,\n",
              " 'yet': 160,\n",
              " 'does': 161,\n",
              " 'nor': 162,\n",
              " 'power': 163,\n",
              " 'too': 164,\n",
              " 'neither': 165,\n",
              " 'able': 166,\n",
              " 'virtue': 167,\n",
              " 'manner': 168,\n",
              " 'whole': 169,\n",
              " 'much': 170,\n",
              " 'body': 171,\n",
              " 'use': 172,\n",
              " 'always': 173,\n",
              " 'themselves': 174,\n",
              " 'guardians': 175,\n",
              " 'go': 176,\n",
              " 'even': 177,\n",
              " 'principle': 178,\n",
              " 'might': 179,\n",
              " 'under': 180,\n",
              " 'tell': 181,\n",
              " 'friend': 182,\n",
              " 'still': 183,\n",
              " 'words': 184,\n",
              " 'rather': 185,\n",
              " 'anything': 186,\n",
              " 'pleasure': 187,\n",
              " 'each': 188,\n",
              " 'ever': 189,\n",
              " 'rulers': 190,\n",
              " 'she': 191,\n",
              " 'ask': 192,\n",
              " 'answer': 193,\n",
              " 'before': 194,\n",
              " 'speaking': 195,\n",
              " 'over': 196,\n",
              " 'clearly': 197,\n",
              " 'socrates': 198,\n",
              " 'come': 199,\n",
              " 'greatest': 200,\n",
              " 'place': 201,\n",
              " 'against': 202,\n",
              " 'class': 203,\n",
              " 'children': 204,\n",
              " 'case': 205,\n",
              " 'further': 206,\n",
              " 'agree': 207,\n",
              " 'among': 208,\n",
              " 'found': 209,\n",
              " 'world': 210,\n",
              " 'kind': 211,\n",
              " 'citizens': 212,\n",
              " 'glaucon': 213,\n",
              " 'light': 214,\n",
              " 'likely': 215,\n",
              " 'pleasures': 216,\n",
              " 'greater': 217,\n",
              " 'god': 218,\n",
              " 'honour': 219,\n",
              " 'desire': 220,\n",
              " 'give': 221,\n",
              " 'hear': 222,\n",
              " 'indeed': 223,\n",
              " 'philosophy': 224,\n",
              " 'thus': 225,\n",
              " 'sight': 226,\n",
              " 'youth': 227,\n",
              " 'less': 228,\n",
              " 'consider': 229,\n",
              " 'view': 230,\n",
              " 'did': 231,\n",
              " 'war': 232,\n",
              " 'desires': 233,\n",
              " 'away': 234,\n",
              " 'end': 235,\n",
              " 'without': 236,\n",
              " 'call': 237,\n",
              " 'money': 238,\n",
              " 'argument': 239,\n",
              " 'interest': 240,\n",
              " 'three': 241,\n",
              " 'wisdom': 242,\n",
              " 'women': 243,\n",
              " 'law': 244,\n",
              " 'education': 245,\n",
              " 'old': 246,\n",
              " 'look': 247,\n",
              " 'believe': 248,\n",
              " 'understand': 249,\n",
              " 'states': 250,\n",
              " 'spirit': 251,\n",
              " 'thrasymachus': 252,\n",
              " 'seen': 253,\n",
              " 'people': 254,\n",
              " 'imagine': 255,\n",
              " 'little': 256,\n",
              " 'else': 257,\n",
              " 'human': 258,\n",
              " 'enough': 259,\n",
              " 'while': 260,\n",
              " 'son': 261,\n",
              " 'where': 262,\n",
              " 'speak': 263,\n",
              " 'could': 264,\n",
              " 'wise': 265,\n",
              " 'love': 266,\n",
              " 'bad': 267,\n",
              " 'point': 268,\n",
              " 'persons': 269,\n",
              " 'thought': 270,\n",
              " 'natural': 271,\n",
              " 'care': 272,\n",
              " 'form': 273,\n",
              " 'work': 274,\n",
              " 'natures': 275,\n",
              " 'friends': 276,\n",
              " 'want': 277,\n",
              " 'rest': 278,\n",
              " 'become': 279,\n",
              " 'music': 280,\n",
              " 'tyrant': 281,\n",
              " 'young': 282,\n",
              " 'opposite': 283,\n",
              " 'truly': 284,\n",
              " 'number': 285,\n",
              " 'arts': 286,\n",
              " 'next': 287,\n",
              " 'together': 288,\n",
              " 'making': 289,\n",
              " 'person': 290,\n",
              " 'find': 291,\n",
              " 'proceed': 292,\n",
              " 'course': 293,\n",
              " 'present': 294,\n",
              " 'individual': 295,\n",
              " 'different': 296,\n",
              " 'common': 297,\n",
              " 'father': 298,\n",
              " 'remember': 299,\n",
              " 'surely': 300,\n",
              " 'homer': 301,\n",
              " 'put': 302,\n",
              " 'really': 303,\n",
              " 'government': 304,\n",
              " 'called': 305,\n",
              " 'eyes': 306,\n",
              " 'last': 307,\n",
              " 'pain': 308,\n",
              " 'sense': 309,\n",
              " 'fear': 310,\n",
              " 'given': 311,\n",
              " 'necessity': 312,\n",
              " 'rule': 313,\n",
              " 'although': 314,\n",
              " 'certain': 315,\n",
              " 'until': 316,\n",
              " 'already': 317,\n",
              " 'qualities': 318,\n",
              " 'third': 319,\n",
              " 'comes': 320,\n",
              " 'beauty': 321,\n",
              " 'done': 322,\n",
              " 'matter': 323,\n",
              " 'hand': 324,\n",
              " 'subject': 325,\n",
              " 'private': 326,\n",
              " 'turn': 327,\n",
              " 'change': 328,\n",
              " 'down': 329,\n",
              " 'between': 330,\n",
              " 'meaning': 331,\n",
              " 'enemies': 332,\n",
              " 'need': 333,\n",
              " 'subjects': 334,\n",
              " 'help': 335,\n",
              " 'adeimantus': 336,\n",
              " 'age': 337,\n",
              " 'drink': 338,\n",
              " 'general': 339,\n",
              " 'name': 340,\n",
              " 'makes': 341,\n",
              " 'idea': 342,\n",
              " 'within': 343,\n",
              " 'here': 344,\n",
              " 'equally': 345,\n",
              " 'stronger': 346,\n",
              " 'says': 347,\n",
              " 'rich': 348,\n",
              " 'whose': 349,\n",
              " 'word': 350,\n",
              " 'means': 351,\n",
              " 'lover': 352,\n",
              " 'real': 353,\n",
              " 'absolute': 354,\n",
              " 'temperance': 355,\n",
              " 'something': 356,\n",
              " 'doing': 357,\n",
              " 'sure': 358,\n",
              " 'though': 359,\n",
              " 'small': 360,\n",
              " 'principles': 361,\n",
              " 'difficulty': 362,\n",
              " 'death': 363,\n",
              " 'similar': 364,\n",
              " 'philosopher': 365,\n",
              " 'appear': 366,\n",
              " 'laws': 367,\n",
              " 'ruler': 368,\n",
              " 'whereas': 369,\n",
              " 'character': 370,\n",
              " 'hardly': 371,\n",
              " 'once': 372,\n",
              " 'heaven': 373,\n",
              " 'exactly': 374,\n",
              " 'begin': 375,\n",
              " 'gymnastic': 376,\n",
              " 'guardian': 377,\n",
              " 'receive': 378,\n",
              " 'lives': 379,\n",
              " 'impossible': 380,\n",
              " 'public': 381,\n",
              " 'live': 382,\n",
              " 'existence': 383,\n",
              " 'possible': 384,\n",
              " 'asked': 385,\n",
              " 'going': 386,\n",
              " 'often': 387,\n",
              " 'poets': 388,\n",
              " 'longer': 389,\n",
              " 'wealth': 390,\n",
              " 'second': 391,\n",
              " 'appears': 392,\n",
              " 'doubt': 393,\n",
              " 'instead': 394,\n",
              " 'hands': 395,\n",
              " 'eye': 396,\n",
              " 'forms': 397,\n",
              " 'science': 398,\n",
              " 'happiness': 399,\n",
              " 'worse': 400,\n",
              " 'full': 401,\n",
              " 'get': 402,\n",
              " 'lie': 403,\n",
              " 'example': 404,\n",
              " 'unless': 405,\n",
              " 'perfectly': 406,\n",
              " 'perfect': 407,\n",
              " 'day': 408,\n",
              " 'difference': 409,\n",
              " 'courage': 410,\n",
              " 'follow': 411,\n",
              " 'grow': 412,\n",
              " 'study': 413,\n",
              " 'years': 414,\n",
              " 'long': 415,\n",
              " 'medicine': 416,\n",
              " 'side': 417,\n",
              " 'taken': 418,\n",
              " 'none': 419,\n",
              " 'supposed': 420,\n",
              " 'allow': 421,\n",
              " 'pure': 422,\n",
              " 'simple': 423,\n",
              " 'object': 424,\n",
              " 'brought': 425,\n",
              " 'earth': 426,\n",
              " 'noble': 427,\n",
              " 'higher': 428,\n",
              " 'necessary': 429,\n",
              " 'beautiful': 430,\n",
              " 'told': 431,\n",
              " 'few': 432,\n",
              " 'wrong': 433,\n",
              " 'seem': 434,\n",
              " 'however': 435,\n",
              " 'according': 436,\n",
              " 'off': 437,\n",
              " 'gain': 438,\n",
              " 'health': 439,\n",
              " 'business': 440,\n",
              " 'philosophers': 441,\n",
              " 'divine': 442,\n",
              " 'imitation': 443,\n",
              " 'democracy': 444,\n",
              " 'towards': 445,\n",
              " 'poor': 446,\n",
              " 'gold': 447,\n",
              " 'enemy': 448,\n",
              " 'allowed': 449,\n",
              " 'above': 450,\n",
              " 'ready': 451,\n",
              " 'times': 452,\n",
              " 'four': 453,\n",
              " 'learn': 454,\n",
              " 'knows': 455,\n",
              " 'enquiry': 456,\n",
              " 'ignorance': 457,\n",
              " 'harmony': 458,\n",
              " 'sun': 459,\n",
              " 'fair': 460,\n",
              " 'experience': 461,\n",
              " 'poet': 462,\n",
              " 'sons': 463,\n",
              " 'reverse': 464,\n",
              " 'proper': 465,\n",
              " 'food': 466,\n",
              " 'dear': 467,\n",
              " 'came': 468,\n",
              " 'quality': 469,\n",
              " 'seeing': 470,\n",
              " 'highest': 471,\n",
              " 'described': 472,\n",
              " 'degree': 473,\n",
              " 'constitution': 474,\n",
              " 'master': 475,\n",
              " 'several': 476,\n",
              " 'happy': 477,\n",
              " 'termed': 478,\n",
              " 'wish': 479,\n",
              " 'admit': 480,\n",
              " 'required': 481,\n",
              " 'compelled': 482,\n",
              " 'vice': 483,\n",
              " 'through': 484,\n",
              " 'beyond': 485,\n",
              " 'individuals': 486,\n",
              " 'image': 487,\n",
              " 'ridiculous': 488,\n",
              " 'hold': 489,\n",
              " 'style': 490,\n",
              " 'becomes': 491,\n",
              " 'bodily': 492,\n",
              " 'term': 493,\n",
              " 'mankind': 494,\n",
              " 'least': 495,\n",
              " 'conceive': 496,\n",
              " 'set': 497,\n",
              " 'beginning': 498,\n",
              " 'show': 499,\n",
              " 'objects': 500,\n",
              " 'describing': 501,\n",
              " 'lovers': 502,\n",
              " 'polemarchus': 503,\n",
              " 'regard': 504,\n",
              " 'evils': 505,\n",
              " 'easily': 506,\n",
              " 'condition': 507,\n",
              " 'physician': 508,\n",
              " 'harm': 509,\n",
              " 'thinking': 510,\n",
              " 'reality': 511,\n",
              " 'admitted': 512,\n",
              " 'observe': 513,\n",
              " 'perhaps': 514,\n",
              " 'ways': 515,\n",
              " 'particular': 516,\n",
              " 'souls': 517,\n",
              " 'poetry': 518,\n",
              " 'imitate': 519,\n",
              " 'faculty': 520,\n",
              " 'shadows': 521,\n",
              " 'satisfied': 522,\n",
              " 'parents': 523,\n",
              " 'disease': 524,\n",
              " 'assuredly': 525,\n",
              " 'heard': 526,\n",
              " 'rightly': 527,\n",
              " 'force': 528,\n",
              " 'miserable': 529,\n",
              " 'house': 530,\n",
              " 'company': 531,\n",
              " 'property': 532,\n",
              " 'below': 533,\n",
              " 'gives': 534,\n",
              " 'result': 535,\n",
              " 'latter': 536,\n",
              " 'nay': 537,\n",
              " 'ignorant': 538,\n",
              " 'generally': 539,\n",
              " 'advantage': 540,\n",
              " 'try': 541,\n",
              " 'strength': 542,\n",
              " 'understanding': 543,\n",
              " 'pains': 544,\n",
              " 'ones': 545,\n",
              " 'maintain': 546,\n",
              " 'unable': 547,\n",
              " 'oligarchy': 548,\n",
              " 'new': 549,\n",
              " 'pass': 550,\n",
              " 'since': 551,\n",
              " 'keep': 552,\n",
              " 'easy': 553,\n",
              " 'cause': 554,\n",
              " 'peace': 555,\n",
              " 'clear': 556,\n",
              " 'act': 557,\n",
              " 'seeking': 558,\n",
              " 'yourself': 559,\n",
              " 'whatever': 560,\n",
              " 'taking': 561,\n",
              " 'authority': 562,\n",
              " 'behold': 563,\n",
              " 'learning': 564,\n",
              " 'praise': 565,\n",
              " 'everything': 566,\n",
              " 'concerned': 567,\n",
              " 'action': 568,\n",
              " 'left': 569,\n",
              " 'discovered': 570,\n",
              " 'classes': 571,\n",
              " 'effect': 572,\n",
              " 'former': 573,\n",
              " 'bring': 574,\n",
              " 'habit': 575,\n",
              " 'woman': 576,\n",
              " 'geometry': 577,\n",
              " 'turned': 578,\n",
              " 'known': 579,\n",
              " 'draw': 580,\n",
              " 'sake': 581,\n",
              " 'useful': 582,\n",
              " 'knowing': 583,\n",
              " 'silver': 584,\n",
              " 'useless': 585,\n",
              " 'agreed': 586,\n",
              " 'danger': 587,\n",
              " 'notion': 588,\n",
              " 'goes': 589,\n",
              " 'tyrannical': 590,\n",
              " 'maker': 591,\n",
              " 'takes': 592,\n",
              " 'slaves': 593,\n",
              " 'single': 594,\n",
              " 'houses': 595,\n",
              " 'kinds': 596,\n",
              " 'animals': 597,\n",
              " 'sorrow': 598,\n",
              " 'passion': 599,\n",
              " 'intermediate': 600,\n",
              " 'visible': 601,\n",
              " 'oligarchical': 602,\n",
              " 'bed': 603,\n",
              " 'round': 604,\n",
              " 'deny': 605,\n",
              " 'freedom': 606,\n",
              " 'country': 607,\n",
              " 'leave': 608,\n",
              " 'actions': 609,\n",
              " 'shown': 610,\n",
              " 'saw': 611,\n",
              " 'escape': 612,\n",
              " 'follows': 613,\n",
              " 'inferior': 614,\n",
              " 'require': 615,\n",
              " 'alone': 616,\n",
              " 'tyranny': 617,\n",
              " 'excellence': 618,\n",
              " 'false': 619,\n",
              " 'minds': 620,\n",
              " 'days': 621,\n",
              " 'upwards': 622,\n",
              " 'trained': 623,\n",
              " 'moment': 624,\n",
              " 'rhythm': 625,\n",
              " 'training': 626,\n",
              " 'element': 627,\n",
              " 'relation': 628,\n",
              " 'share': 629,\n",
              " 'pursuits': 630,\n",
              " 'gifts': 631,\n",
              " 'perceive': 632,\n",
              " 'tale': 633,\n",
              " 'bear': 634,\n",
              " 'pilot': 635,\n",
              " 'excellent': 636,\n",
              " 'seems': 637,\n",
              " 'angry': 638,\n",
              " 'ruling': 639,\n",
              " 'acknowledge': 640,\n",
              " 'mans': 641,\n",
              " 'deemed': 642,\n",
              " 'judge': 643,\n",
              " 'fight': 644,\n",
              " 'assume': 645,\n",
              " 'utterly': 646,\n",
              " 'middle': 647,\n",
              " 'strain': 648,\n",
              " 'sorts': 649,\n",
              " 'educated': 650,\n",
              " 'influence': 651,\n",
              " 'naturally': 652,\n",
              " 'spoke': 653,\n",
              " 'images': 654,\n",
              " 'home': 655,\n",
              " 'enquire': 656,\n",
              " 'feeling': 657,\n",
              " 'near': 658,\n",
              " 'tales': 659,\n",
              " 'pay': 660,\n",
              " 'back': 661,\n",
              " 'spoken': 662,\n",
              " 'alike': 663,\n",
              " 'command': 664,\n",
              " 'play': 665,\n",
              " 'account': 666,\n",
              " 'ill': 667,\n",
              " 'itself': 668,\n",
              " 'names': 669,\n",
              " 'whenever': 670,\n",
              " 'worst': 671,\n",
              " 'affirm': 672,\n",
              " 'ground': 673,\n",
              " 'suffer': 674,\n",
              " 'judgment': 675,\n",
              " 'honours': 676,\n",
              " 'describe': 677,\n",
              " 'mother': 678,\n",
              " 'painter': 679,\n",
              " 'original': 680,\n",
              " 'ourselves': 681,\n",
              " 'sees': 682,\n",
              " 'zeus': 683,\n",
              " 'parts': 684,\n",
              " 'military': 685,\n",
              " 'possibility': 686,\n",
              " 'virtues': 687,\n",
              " 'sciences': 688,\n",
              " 'received': 689,\n",
              " 'thinks': 690,\n",
              " 'child': 691,\n",
              " 'attempt': 692,\n",
              " 'duty': 693,\n",
              " 'interests': 694,\n",
              " 'sometimes': 695,\n",
              " 'mistaken': 696,\n",
              " 'respect': 697,\n",
              " 'fails': 698,\n",
              " 'its': 699,\n",
              " 'office': 700,\n",
              " 'free': 701,\n",
              " 'considered': 702,\n",
              " 'honourable': 703,\n",
              " 'carried': 704,\n",
              " 'lest': 705,\n",
              " 'gentle': 706,\n",
              " 'appearance': 707,\n",
              " 'causes': 708,\n",
              " 'dog': 709,\n",
              " 'parent': 710,\n",
              " 'attain': 711,\n",
              " 'dialectic': 712,\n",
              " 'run': 713,\n",
              " 'arrived': 714,\n",
              " 'sweet': 715,\n",
              " 'undoubtedly': 716,\n",
              " 'dogs': 717,\n",
              " 'battle': 718,\n",
              " 'discussion': 719,\n",
              " 'willing': 720,\n",
              " 'fact': 721,\n",
              " 'numbers': 722,\n",
              " 'cases': 723,\n",
              " 'payment': 724,\n",
              " 'utter': 725,\n",
              " 'conclusion': 726,\n",
              " 'discover': 727,\n",
              " 'mere': 728,\n",
              " 'wants': 729,\n",
              " 'origin': 730,\n",
              " 'hearing': 731,\n",
              " 'remains': 732,\n",
              " 'large': 733,\n",
              " 'reasons': 734,\n",
              " 'unlike': 735,\n",
              " 'please': 736,\n",
              " 'add': 737,\n",
              " 'purpose': 738,\n",
              " 'rewards': 739,\n",
              " 'liberty': 740,\n",
              " 'king': 741,\n",
              " 'dead': 742,\n",
              " 'finger': 743,\n",
              " 'cities': 744,\n",
              " 'process': 745,\n",
              " 'land': 746,\n",
              " 'especially': 747,\n",
              " 'author': 748,\n",
              " 'matters': 749,\n",
              " 'previous': 750,\n",
              " 'lot': 751,\n",
              " 'meet': 752,\n",
              " 'harmonies': 753,\n",
              " 'drawn': 754,\n",
              " 'equal': 755,\n",
              " 'intelligence': 756,\n",
              " 'proportion': 757,\n",
              " 'hellenes': 758,\n",
              " 'measure': 759,\n",
              " 'thirst': 760,\n",
              " 'division': 761,\n",
              " 'pursuit': 762,\n",
              " 'distance': 763,\n",
              " 'coming': 764,\n",
              " 'listen': 765,\n",
              " 'soon': 766,\n",
              " 'journey': 767,\n",
              " 'fault': 768,\n",
              " 'answered': 769,\n",
              " 'possess': 770,\n",
              " 'profit': 771,\n",
              " 'possession': 772,\n",
              " 'sacrifices': 773,\n",
              " 'probably': 774,\n",
              " 'giving': 775,\n",
              " 'guilty': 776,\n",
              " 'mistake': 777,\n",
              " 'myself': 778,\n",
              " 'weaker': 779,\n",
              " 'absolutely': 780,\n",
              " 'liable': 781,\n",
              " 'contrary': 782,\n",
              " 'fail': 783,\n",
              " 'deal': 784,\n",
              " 'got': 785,\n",
              " 'entirely': 786,\n",
              " 'ideas': 787,\n",
              " 'getting': 788,\n",
              " 'ears': 789,\n",
              " 'choose': 790,\n",
              " 'incapable': 791,\n",
              " 'unity': 792,\n",
              " 'explain': 793,\n",
              " 'necessarily': 794,\n",
              " 'practise': 795,\n",
              " 'arise': 796,\n",
              " 'lead': 797,\n",
              " 'along': 798,\n",
              " 'continue': 799,\n",
              " 'fourth': 800,\n",
              " 'water': 801,\n",
              " 'husbandman': 802,\n",
              " 'motion': 803,\n",
              " 'destroy': 804,\n",
              " 'elements': 805,\n",
              " 'aware': 806,\n",
              " 'imitator': 807,\n",
              " 'fall': 808,\n",
              " 'soldiers': 809,\n",
              " 'strange': 810,\n",
              " 'sphere': 811,\n",
              " 'fathers': 812,\n",
              " 'terms': 813,\n",
              " 'essence': 814,\n",
              " 'wives': 815,\n",
              " 'notbeing': 816,\n",
              " 'offer': 817,\n",
              " 'took': 818,\n",
              " 'persuade': 819,\n",
              " 'carry': 820,\n",
              " 'head': 821,\n",
              " 'difficult': 822,\n",
              " 'feel': 823,\n",
              " 'fortune': 824,\n",
              " 'hence': 825,\n",
              " 'begins': 826,\n",
              " 'hope': 827,\n",
              " 'occasion': 828,\n",
              " 'greatly': 829,\n",
              " 'advantages': 830,\n",
              " 'o': 831,\n",
              " 'meant': 832,\n",
              " 'musician': 833,\n",
              " 'benefit': 834,\n",
              " 'clearness': 835,\n",
              " 'hard': 836,\n",
              " 'answers': 837,\n",
              " 'dare': 838,\n",
              " 'notions': 839,\n",
              " 'acknowledged': 840,\n",
              " 'artist': 841,\n",
              " 'distinguished': 842,\n",
              " 'blessed': 843,\n",
              " 'fairly': 844,\n",
              " 'passed': 845,\n",
              " 'proof': 846,\n",
              " 'began': 847,\n",
              " 'special': 848,\n",
              " 'laid': 849,\n",
              " 'trouble': 850,\n",
              " 'charming': 851,\n",
              " 'profitable': 852,\n",
              " 'rate': 853,\n",
              " 'creature': 854,\n",
              " 'exist': 855,\n",
              " 'army': 856,\n",
              " 'appointed': 857,\n",
              " 'lost': 858,\n",
              " 'apply': 859,\n",
              " 'taste': 860,\n",
              " 'voice': 861,\n",
              " 'honoured': 862,\n",
              " 'service': 863,\n",
              " 'wonder': 864,\n",
              " 'likeness': 865,\n",
              " 'dream': 866,\n",
              " 'vision': 867,\n",
              " 'courageous': 868,\n",
              " 'carpenter': 869,\n",
              " 'kings': 870,\n",
              " 'slave': 871,\n",
              " 'nearly': 872,\n",
              " 'discord': 873,\n",
              " 'corruption': 874,\n",
              " 'passionate': 875,\n",
              " 'ruin': 876,\n",
              " 'upper': 877,\n",
              " 'community': 878,\n",
              " 'figure': 879,\n",
              " 'appeared': 880,\n",
              " 'remain': 881,\n",
              " 'refuse': 882,\n",
              " 'horses': 883,\n",
              " 'convinced': 884,\n",
              " 'famous': 885,\n",
              " 'reply': 886,\n",
              " 'reflect': 887,\n",
              " 'arms': 888,\n",
              " 'return': 889,\n",
              " 'senses': 890,\n",
              " 'bodies': 891,\n",
              " 'guard': 892,\n",
              " 'expected': 893,\n",
              " 'wild': 894,\n",
              " 'beast': 895,\n",
              " 'fixed': 896,\n",
              " 'length': 897,\n",
              " 'wholly': 898,\n",
              " 'democratical': 899,\n",
              " 'remark': 900,\n",
              " 'used': 901,\n",
              " 'skill': 902,\n",
              " 'questions': 903,\n",
              " 'future': 904,\n",
              " 'superior': 905,\n",
              " 'provide': 906,\n",
              " 'aim': 907,\n",
              " 'perfection': 908,\n",
              " 'fancy': 909,\n",
              " 'learned': 910,\n",
              " 'determine': 911,\n",
              " 'lesser': 912,\n",
              " 'assent': 913,\n",
              " 'various': 914,\n",
              " 'held': 915,\n",
              " 'knew': 916,\n",
              " 'serious': 917,\n",
              " 'goods': 918,\n",
              " 'judges': 919,\n",
              " 'strong': 920,\n",
              " 'earnest': 921,\n",
              " 'otherwise': 922,\n",
              " 'choice': 923,\n",
              " 'relative': 924,\n",
              " 'quarrel': 925,\n",
              " 'family': 926,\n",
              " 'except': 927,\n",
              " 'blind': 928,\n",
              " 'misery': 929,\n",
              " 'passing': 930,\n",
              " 'reputation': 931,\n",
              " 'worthy': 932,\n",
              " 'chosen': 933,\n",
              " 'becoming': 934,\n",
              " 'heavens': 935,\n",
              " 'forth': 936,\n",
              " 'living': 937,\n",
              " 'numerous': 938,\n",
              " 'heroes': 939,\n",
              " 'afraid': 940,\n",
              " 'offspring': 941,\n",
              " 'larger': 942,\n",
              " 'mention': 943,\n",
              " 'labour': 944,\n",
              " 'multitude': 945,\n",
              " 'warrior': 946,\n",
              " 'duties': 947,\n",
              " 'spirited': 948,\n",
              " 'gymnastics': 949,\n",
              " 'external': 950,\n",
              " 'circumstances': 951,\n",
              " 'fairest': 952,\n",
              " 'temperate': 953,\n",
              " 'habits': 954,\n",
              " 'asclepius': 955,\n",
              " 'happen': 956,\n",
              " 'ideal': 957,\n",
              " 'holds': 958,\n",
              " 'legislator': 959,\n",
              " 'appetites': 960,\n",
              " 'akin': 961,\n",
              " 'birth': 962,\n",
              " 'hypotheses': 963,\n",
              " 'remaining': 964,\n",
              " 'direction': 965,\n",
              " 'servant': 966,\n",
              " 'brother': 967,\n",
              " 'commonly': 968,\n",
              " 'suspect': 969,\n",
              " 'besides': 970,\n",
              " 'expect': 971,\n",
              " 'sum': 972,\n",
              " 'sleep': 973,\n",
              " 'debt': 974,\n",
              " 'amid': 975,\n",
              " 'inference': 976,\n",
              " 'argue': 977,\n",
              " 'utmost': 978,\n",
              " 'avoid': 979,\n",
              " 'obey': 980,\n",
              " 'sailors': 981,\n",
              " 'exercise': 982,\n",
              " 'suitable': 983,\n",
              " 'taught': 984,\n",
              " 'shepherd': 985,\n",
              " 'scale': 986,\n",
              " 'ambitious': 987,\n",
              " 'task': 988,\n",
              " 'propose': 989,\n",
              " 'practice': 990,\n",
              " 'disposed': 991,\n",
              " 'freemen': 992,\n",
              " 'accomplished': 993,\n",
              " 'deprived': 994,\n",
              " 'assigned': 995,\n",
              " 'watch': 996,\n",
              " 'iron': 997,\n",
              " 'step': 998,\n",
              " 'hour': 999,\n",
              " 'lying': 1000,\n",
              " ...}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWagP5ehJCGL"
      },
      "source": [
        "<a id=section603></a>\n",
        "### **6.3 Sequence Inputs and Output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PACdSAPYJAbT",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# separate into input and output\n",
        "sequences = array(sequences) #array slicing\n",
        "\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "\n",
        "#one hot encode the output word.\n",
        "#Keras provides the to_categorical() that can be used to one hot encode the output words for each input-output sequence pair.\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugQoKt9G3FHe",
        "outputId": "fb644405-035f-4e22-bc0c-a6c4898f9bc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(118633, 50)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WbqBTEc3FHm",
        "outputId": "c17efb1e-543c-4a37-cb55-663636572b69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1046,   11,   11, 1045,  329, 7409,    4,    1, 2873,   35,  213,\n",
              "          1,  261,    3, 2251,    9,   11,  179,  817,  123,   92, 2872,\n",
              "          4,    1, 2249, 7408,    1, 7407, 7406,    2,   75,  120,   11,\n",
              "       1266,    4,  110,    6,   30,  168,   16,   49, 7405,    1, 1609,\n",
              "         13,   57,    8,  549,  151,   11])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuQ3PYYDqveu",
        "outputId": "6297bca2-7b0f-45b9-e106-4ad3d450e3a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "easmWXJu3FHv",
        "outputId": "5505afdc-616c-4981-f05c-6d92d1ab0e45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(118633, 7410)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G_pEZAf3FHy",
        "outputId": "9afcd0ed-36b9-499b-d9df-2f6f3e374987"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9PvtEQA3FH0"
      },
      "source": [
        "<a id=section604></a>\n",
        "### **6.4 Fit Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zw5tiv7JKwX"
      },
      "source": [
        "- The learned **embedding** needs to know the size of the **vocabulary** and the length of **input sequences** as previously discussed.\n",
        "\n",
        " - The **output layer** predicts the **next word** as a single **vector** the size of the **vocabulary** with a **probability** for each word in the vocabulary.\n",
        "\n",
        " - A **softmax** activation function is used to **ensure** the outputs have the **characteristics** of normalized probabilities.\n",
        "\n",
        " <center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/images.png\"width=\"400\" height=\"150\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "XmzS7OUuJIps",
        "outputId": "3eb76106-825a-4d9c-b7c2-2900d47fb7bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 50)            370500    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 7410)              748410    \n",
            "=================================================================\n",
            "Total params: 1,269,810\n",
            "Trainable params: 1,269,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "\"\"\"\"\n",
        "- Size of the **embedding** vector space: a parameter to specify how many dimensions will be used to represent each word\n",
        "\n",
        "- Common values are **50, 100, and 300**.\n",
        "\n",
        "- We will use 50 here, but consider **testing smaller or larger values**.\n",
        "\n",
        "- We will use a two LSTM hidden layers with **100 memory cells** each.\n",
        "\n",
        "- More memory cells and a deeper network may achieve better results.\n",
        "\"\"\"\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbnFr60_l72M"
      },
      "outputs": [],
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-2JH1NlJYcW"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "- The model is compiled specifying the **categorical** cross **entropy** loss needed to fit the **model**.\n",
        "\n",
        "- Technically, the **model** is learning a **multi-class** classification and this is the **suitable** loss function for this type of problem.\n",
        "\n",
        "- The efficient **Adam** optimizers to **mini-batch** gradient descent is used and **accuracy** is evaluated of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3XPocfhlqpM"
      },
      "source": [
        "- **Model Training** on the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "zrgQUag_JUwW",
        "outputId": "dc9645da-01b2-4484-d34f-45b235a7a6e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "118633/118633 [==============================] - 138s 1ms/step - loss: 6.1902 - acc: 0.0666\n",
            "Epoch 2/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 5.7708 - acc: 0.0983\n",
            "Epoch 3/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 5.5591 - acc: 0.1187\n",
            "Epoch 4/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 5.4088 - acc: 0.1306\n",
            "Epoch 5/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 5.2885 - acc: 0.1421\n",
            "Epoch 6/100\n",
            "118633/118633 [==============================] - 133s 1ms/step - loss: 5.1942 - acc: 0.1520\n",
            "Epoch 7/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 5.1165 - acc: 0.1552\n",
            "Epoch 8/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 5.0469 - acc: 0.1596\n",
            "Epoch 9/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.9811 - acc: 0.1620\n",
            "Epoch 10/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.9142 - acc: 0.1677\n",
            "Epoch 11/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 4.8508 - acc: 0.1722\n",
            "Epoch 12/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 4.7896 - acc: 0.1772\n",
            "Epoch 13/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.7338 - acc: 0.1816\n",
            "Epoch 14/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.6806 - acc: 0.1848\n",
            "Epoch 15/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.6422 - acc: 0.1866\n",
            "Epoch 16/100\n",
            "118633/118633 [==============================] - 133s 1ms/step - loss: 4.7516 - acc: 0.1778\n",
            "Epoch 17/100\n",
            "118633/118633 [==============================] - 132s 1ms/step - loss: 4.6509 - acc: 0.1839\n",
            "Epoch 18/100\n",
            "118633/118633 [==============================] - 130s 1ms/step - loss: 4.5721 - acc: 0.1915\n",
            "Epoch 19/100\n",
            "118633/118633 [==============================] - 130s 1ms/step - loss: 4.5173 - acc: 0.1951\n",
            "Epoch 20/100\n",
            "118633/118633 [==============================] - 129s 1ms/step - loss: 4.4630 - acc: 0.1984\n",
            "Epoch 21/100\n",
            "118633/118633 [==============================] - 129s 1ms/step - loss: 4.4135 - acc: 0.2020\n",
            "Epoch 22/100\n",
            "118633/118633 [==============================] - 130s 1ms/step - loss: 4.3711 - acc: 0.2035\n",
            "Epoch 23/100\n",
            "118633/118633 [==============================] - 129s 1ms/step - loss: 4.3291 - acc: 0.2065\n",
            "Epoch 24/100\n",
            "118633/118633 [==============================] - 130s 1ms/step - loss: 4.2663 - acc: 0.2108\n",
            "Epoch 25/100\n",
            "118633/118633 [==============================] - 133s 1ms/step - loss: 4.2134 - acc: 0.2140\n",
            "Epoch 26/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.1783 - acc: 0.2154\n",
            "Epoch 27/100\n",
            "118633/118633 [==============================] - 133s 1ms/step - loss: 4.1498 - acc: 0.2175\n",
            "Epoch 28/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.1175 - acc: 0.2194\n",
            "Epoch 29/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 4.1845 - acc: 0.2123\n",
            "Epoch 30/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.1224 - acc: 0.2175\n",
            "Epoch 31/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.0775 - acc: 0.2199\n",
            "Epoch 32/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 4.0386 - acc: 0.2238\n",
            "Epoch 33/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.9981 - acc: 0.2271\n",
            "Epoch 34/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.9668 - acc: 0.2291\n",
            "Epoch 35/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.9288 - acc: 0.2335\n",
            "Epoch 36/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.8928 - acc: 0.2350\n",
            "Epoch 37/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.8631 - acc: 0.2390\n",
            "Epoch 38/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.8343 - acc: 0.2418\n",
            "Epoch 39/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.8917 - acc: 0.2366\n",
            "Epoch 40/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.8872 - acc: 0.2353\n",
            "Epoch 41/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.8548 - acc: 0.2410\n",
            "Epoch 42/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.8132 - acc: 0.2437\n",
            "Epoch 43/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.7707 - acc: 0.2491\n",
            "Epoch 44/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.7625 - acc: 0.2497\n",
            "Epoch 45/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.7447 - acc: 0.2525\n",
            "Epoch 46/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.7106 - acc: 0.2544\n",
            "Epoch 47/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.6790 - acc: 0.2575\n",
            "Epoch 48/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.7335 - acc: 0.2518\n",
            "Epoch 49/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 3.6997 - acc: 0.2569\n",
            "Epoch 50/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.6680 - acc: 0.2609\n",
            "Epoch 51/100\n",
            "118633/118633 [==============================] - 138s 1ms/step - loss: 3.6329 - acc: 0.2647\n",
            "Epoch 52/100\n",
            "118633/118633 [==============================] - 138s 1ms/step - loss: 3.6061 - acc: 0.2668\n",
            "Epoch 53/100\n",
            "118633/118633 [==============================] - 139s 1ms/step - loss: 3.5709 - acc: 0.2716\n",
            "Epoch 54/100\n",
            "118633/118633 [==============================] - 137s 1ms/step - loss: 3.5669 - acc: 0.2720\n",
            "Epoch 55/100\n",
            "118633/118633 [==============================] - 138s 1ms/step - loss: 3.5531 - acc: 0.2743\n",
            "Epoch 56/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.5136 - acc: 0.2777\n",
            "Epoch 57/100\n",
            "118633/118633 [==============================] - 133s 1ms/step - loss: 3.4769 - acc: 0.2824\n",
            "Epoch 58/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.4474 - acc: 0.2864\n",
            "Epoch 59/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.4185 - acc: 0.2910\n",
            "Epoch 60/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.3894 - acc: 0.2948\n",
            "Epoch 61/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.3698 - acc: 0.2966\n",
            "Epoch 62/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 3.3471 - acc: 0.3001\n",
            "Epoch 63/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.3651 - acc: 0.2989\n",
            "Epoch 64/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 3.4428 - acc: 0.2901\n",
            "Epoch 65/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.3848 - acc: 0.2963\n",
            "Epoch 66/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.3212 - acc: 0.3057\n",
            "Epoch 67/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.2815 - acc: 0.3092\n",
            "Epoch 68/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.2498 - acc: 0.3147\n",
            "Epoch 69/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.2380 - acc: 0.3149\n",
            "Epoch 70/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.1985 - acc: 0.3230\n",
            "Epoch 71/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.2265 - acc: 0.3186\n",
            "Epoch 72/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.1872 - acc: 0.3230\n",
            "Epoch 73/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.1611 - acc: 0.3269\n",
            "Epoch 74/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.1219 - acc: 0.3326\n",
            "Epoch 75/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.0906 - acc: 0.3379\n",
            "Epoch 76/100\n",
            "118633/118633 [==============================] - 137s 1ms/step - loss: 3.0649 - acc: 0.3422\n",
            "Epoch 77/100\n",
            "118633/118633 [==============================] - 139s 1ms/step - loss: 3.0385 - acc: 0.3442\n",
            "Epoch 78/100\n",
            "118633/118633 [==============================] - 140s 1ms/step - loss: 3.0094 - acc: 0.3496\n",
            "Epoch 79/100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "118633/118633 [==============================] - 138s 1ms/step - loss: 2.9889 - acc: 0.3539\n",
            "Epoch 80/100\n",
            "118633/118633 [==============================] - 138s 1ms/step - loss: 2.9617 - acc: 0.3571\n",
            "Epoch 81/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 2.9625 - acc: 0.3575\n",
            "Epoch 82/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 2.9518 - acc: 0.3605\n",
            "Epoch 83/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 2.9235 - acc: 0.3651\n",
            "Epoch 84/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.9103 - acc: 0.3675\n",
            "Epoch 85/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 2.9091 - acc: 0.3675\n",
            "Epoch 86/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.9021 - acc: 0.3698\n",
            "Epoch 87/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 2.8361 - acc: 0.3787\n",
            "Epoch 88/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 2.8087 - acc: 0.3838\n",
            "Epoch 89/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.7992 - acc: 0.3850\n",
            "Epoch 90/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.8173 - acc: 0.3846\n",
            "Epoch 91/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.7961 - acc: 0.3879\n",
            "Epoch 92/100\n",
            "118633/118633 [==============================] - 137s 1ms/step - loss: 2.7965 - acc: 0.3894\n",
            "Epoch 93/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.8260 - acc: 0.3887\n",
            "Epoch 94/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.7935 - acc: 0.3919\n",
            "Epoch 95/100\n",
            "113024/118633 [===========================>..] - ETA: 6s - loss: 3.0228 - acc: 0.3685"
          ]
        }
      ],
      "source": [
        "\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F85vCF63PGgo"
      },
      "source": [
        "- Use the **Keras model API** to save the model to the file **â€˜model.h5â€˜** in the current working directory.\n",
        "\n",
        "- This is in the **Tokenizer object**, and we can save that too **using Pickle**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TG1hkckPMQg"
      },
      "outputs": [],
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTVXdfzw3FIN"
      },
      "source": [
        "----\n",
        "<a id=section7></a>\n",
        "## **7. Use Language model**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq3Xy3B23FIO"
      },
      "source": [
        "\n",
        "<br>   \n",
        "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/word_lstm_flow10.png\"width=\"700\" height=\"400\"/></center>\n",
        "\n",
        "<br>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLOxH41PQlx"
      },
      "source": [
        "<a id=section701></a>\n",
        "### **7.1 Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwtQz4N3PZ-1"
      },
      "outputs": [],
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load cleaned text sequences\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDiucNmFPd02"
      },
      "source": [
        "- We need the text so that we can choose a **source sequence** as input to the model for generating a **new sequence of text**.\n",
        "\n",
        "- The model will require **50 words** as **input**.\n",
        "\n",
        "- Later, we will need to specify the **expected length** of input.\n",
        "\n",
        "- We can determine this from the **input sequences** by **calculating** the length of one line of the loaded data and **subtracting** **1** for the **expected output** word that is also on the same line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_uNuSY7PhVP"
      },
      "outputs": [],
      "source": [
        "seq_length = len(lines[0].split()) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUEry7f1PjtJ"
      },
      "source": [
        "<a id=section702></a>\n",
        "### **7.2 Load Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3G2SzwJ3FId"
      },
      "source": [
        "- We can now **load the model** from file.\n",
        "\n",
        "\n",
        "- Keras provides the **load_model() function** for loading the model, ready for use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAIRVWtaPp2l"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjdIpsuK3FIg"
      },
      "source": [
        "<a id=section703></a>\n",
        "### **7.3 Generate Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKnsr61-Puwe"
      },
      "source": [
        "* The first step in generating text is **preparing a seed input**.\n",
        "\n",
        "\n",
        "* We will select a **random line** of text from the **input text** for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvyF0mPZHcy2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = list()\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of words\n",
        "    for i in range(n_words):\n",
        "        # encode the text as integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict probabilities for each word\n",
        "        yhat = model.predict_classes(encoded, verbose=0)\n",
        "        # map predicted word index to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)\n",
        "\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "\n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(\"seed_text:\" + '\\n')\n",
        "print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "print(\"generated_text:\" + '\\n')\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3SvYiMo3FIm"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        " - In fact, the **addition** of **concatenation** would help in interpreting the seed and the **generated** text. Nevertheless, the **generated** text gets the right kind of words in the **right** kind of order.\n",
        "\n",
        " - Try running the **example** a few times to see other examples of **generated** text. Let me know in the **comments** below if you see anything interesting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ViQrJNdLe6_"
      },
      "source": [
        "----\n",
        "<a id=section8></a>\n",
        "## **8. Conclusion**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-oYCORA4oCf",
        "toc-hr-collapsed": false
      },
      "source": [
        "- That **statistical** language models are **central** to many challenging natural language processing tasks.\n",
        "\n",
        "- That state-of-the-art **results** are achieved using **neural language models**, specifically those with **word embeddings** and recurrent neural network algorithms.\n",
        "\n",
        "- In general, **word-level language** models tend to **display** higher accuracy than **character-level language models**.\n",
        "\n",
        "- This is because they can form **shorter** representations of **sentences** and preserve the **context between** words easier than character-level language models.\n",
        "\n",
        "- They allow **conditioning** on increasingly large **context** sizes with only a linear increase in the number of parameters, and they support generalization across **different** contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSqqs8h0QWig"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
